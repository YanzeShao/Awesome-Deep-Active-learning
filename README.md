# Awesome Deep Active learning  
## What is Deep Active Learning(DAL)?  
Deep Active Learning (DeepAL) is a research area that combines active learning (AL) and deep learning (DL). Active learning aims to maximize a model’s performance gain while annotating the fewest samples possible. Given that deep learning requires a large amount of data to optimize a massive number of parameters, it's been proposed that active learning could be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of deep learning. This intersection of active learning and deep learning is referred to as deep active learning​.
### Active Learning
Active learning is a special machine learning method, aiming to achieve maximum learning efficiency with as few samples as possible. In many practical scenarios, obtaining a large amount of labeled data is both time-consuming and labor-intensive, especially in areas requiring specialized knowledge for annotation, such as medical image recognition or speech recognition. Active learning efficiently reduces the annotation cost by allowing the model to select samples that are most likely to improve its performance for labeling.  
Active learning methods are categorized based on application scenarios:  
* membership query synthesis: the learner requests labeling of any unlabeled samples, including samples randomly generated by the learner itself
* stream-based selective sampling: sequentially examines samples based on a query strategy to determine if they need to be labeled; suitable for devices with time complexity constraints such as mobile phones
* pool-based AL: examines and ranks the entire dataset at once, identifying a batch of unlabeled samples
Query strategies include:
* uncertainty-based approach
* diversity-based approach
* expected model change
* Diversity-based and density-based query strategies
* Uncertainty-based query strategy
* hybrid query strategies: balancing uncertainty & diversity

Focusing solely on uncertainty may result in bias during sampling as the selected samples might not be representative of the entire unlabeled dataset; while focusing only on diversity might lead to the collection of many inefficient samples, thereby increasing the overall annotation cost. 
<br>  
<br>
Current active learning works are concentrated in low-dimensional data, and there are issues when applied to high-dimensional data such as text and images. Pure active learning has several problems:
* Using Active learning along is not as effective as other algorithms, such as semi-supervised learning
* Problems arising when integrating with other components
* Data augmentation only using random flipping and cropping, but another major problem with active learning is that it cannot effectively integrate with strong data augmentation
### COMBINING DL AND AL
Deep Learning excels at extracting features in high-dimensional data, while active learning can reduce annotation costs, hence their strengths are complementary, forming deep active learning.  
Deep active learning is widely applied in various fields because it can efficiently handle high-dimensional data and improve model performance with limited labeled data. Below are some specific application areas for deep active learning:  
* Image recognition and processing: In image recognition and processing, deep active learning is used to select samples with the highest informational value for labeling, in order to improve the performance of the model.
* Natural language processing: In natural language processing, deep active learning can help improve performance in tasks such as text classification, named entity recognition, sentiment analysis, especially when there is limited labeled data.
* Medical image analysis: In medical image analysis, deep active learning can help select the most challenging images (e.g., those that are difficult for the model to classify) for expert annotation, thereby improving the model’s performance in complex and uncertain tasks.
* Autonomous driving and robotics: In the fields of autonomous driving and robotics, deep active learning can help systems learn more effectively from the environment, for example, by selecting data samples that can best improve navigation or control strategies.
* Scientific Computing

## Important SurveYeary Papers  
| Year     | Title     | Venue | Paper | Code |
| :-------: | ------- | :-------: | :-------: | :-------: |
| 2020 | **A Survey of Deep Active Learning** | arXiv |[Link](https://arxiv.org/abs/2009.00236) |-|
| 2021 | **A survey on active learning and human-in-the-loop deep learning for medical image analysis**  | MedIA |[Link](https://www.sciencedirect.com/science/article/abs/pii/S1361841521001080)|-|
| 2022 | **Deep Bayesian Active Learning, A Brief Survey on Recent Advances**  | arXiv |[Link](https://arxiv.org/abs/2012.08044)|-|
| 2022 | **Deep Active Learning for Computer Vision: Past and Future**  | arXiv |[Link](https://arxiv.org/abs/2211.14819)|-|
| 2022 | **A Comparative Survey of Deep Active Learning**  | arXiv |[Link](https://arxiv.org/abs/2203.13450) |[Code](https://github.com/SineZHAN/deepALplus)|
| 2023 | **Deep Active Learning in the Presence of Label Noise: A Survey**  | arXiv |[Link](https://arxiv.org/abs/2302.11075) |-|


## Papers
| Year     | Title     | Venue | Paper | Code |
| :-------: | ------- | :-------: | :-------: | :-------: |
|2017| **Deep Active Learning for Dialogue Generation**   |arXiv|[Link](https://arxiv.org/abs/1612.03929)|-|
|2017| **Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation**   |arXiv|[Link](https://arxiv.org/abs/1706.04737)|-|
|2018| **Deep Active Learning for Named Entity Recognition**   |arXiv|[Link](https://arxiv.org/abs/1707.05928)|-|
|2019| **Deep Active Learning for Axon-Myelin Segmentation on Histology Data** |arXiv|[Link](https://arxiv.org/abs/1907.05143)|[Code](https://github.com/neuropoly/deep-active-learning)|
|2020| **Deep active learning for object detection**   |INS|[Link](https://www.sciencedirect.com/science/article/abs/pii/S0020025521008197)|-|
|2020| **Deep active learning for object detection**   |BMVC|[Link](http://www.bmva.org/bmvc/2018/contents/papers/0287.pdf)|-|
|2022| **Towards Robust Deep Active Learning for Scientific Computing**   |arXiv|[Link](https://arxiv.org/abs/2201.12632)|-|
|2022| **Active Learning with Neural Networks: Insights from Nonparametric Statistics**   |NeurIPS|[Link](https://openreview.net/pdf?id=LRMmgkcoCnW)|-|


### CVPR
| Year     | Title     | Venue | Paper | Code |
| :-------: | ------- | :-------: | :-------: | :-------: |
|2020| **Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision**   |CVPR|[Link](https://openaccess.thecvf.com/content_CVPR_2020/papers/Gudovskiy_Deep_Active_Learning_for_Biased_Datasets_via_Fisher_Kernel_Self-Supervision_CVPR_2020_paper.pdf)|[Code](https://github.com/gudovskiy/al-fk-self-supervision)|
|2021| **Sequential Graph Convolutional Network for Active Learning**   |CVPR|[Link](https://openaccess.thecvf.com/content/CVPR2021/papers/Caramalau_Sequential_Graph_Convolutional_Network_for_Active_Learning_CVPR_2021_paper.pdf)|-|
|2021| **VaB-AL: Incorporating Class Imbalance and Difficulty With Variational Bayes for Active Learning**   |CVPR|[Link](https://openaccess.thecvf.com/content/CVPR2021/papers/Choi_VaB-AL_Incorporating_Class_Imbalance_and_Difficulty_With_Variational_Bayes_for_CVPR_2021_paper.pdf)|-|
|2021| **Transferable Query Selection for Active Domain Adaptation**   |CVPR|[Link](https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Transferable_Query_Selection_for_Active_Domain_Adaptation_CVPR_2021_paper.pdf)|-|
|2021| **Multiple Instance Active Learning for Object Detection**   |CVPR|[Link](https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multiple_Instance_Active_Learning_for_Object_Detection_CVPR_2021_paper.pdf)|-|
|2021| **Task-Aware Variational Adversarial Active Learning**   |CVPR|[Link](https://arxiv.org/abs/2002.04709)|-|
|2021| **Revisiting Superpixels for Active Learning in Semantic Segmentation With Realistic Annotation Costs**   |CVPR|[Link](https://openaccess.thecvf.com/content/CVPR2021/papers/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.pdf)|-|

### ICCV
| Year     | Title     | Venue | Paper | Code |
| :-------: | ------- | :-------: | :-------: | :-------: |
|2017| **Active Decision Boundary Annotation with Deep Generative Models**   |ICCV|[Link](https://arxiv.org/abs/1703.06971)|[Code](https://github.com/MiriamHu/ActiveBoundary)|
|2021| **S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation**   |ICCV|[Link](https://openaccess.thecvf.com/content/ICCV2021/papers/Rangwani_S3VAADA_Submodular_Subset_Selection_for_Virtual_Adversarial_Active_Domain_Adaptation_ICCV_2021_paper.pdf)|[Code](https://github.com/val-iisc/s3vaada)|
|2021| **Active Universal Domain Adaptation**   |ICCV|[Link](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.pdf)|-|
|2021| **Influence Selection for Active Learning**   |ICCV|[Link](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Influence_Selection_for_Active_Learning_ICCV_2021_paper.pdf)|-|
|2021| **Active Learning for Deep Object Detection via Probabilistic Modeling**   |ICCV|[Link](https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Active_Learning_for_Deep_Object_Detection_via_Probabilistic_Modeling_ICCV_2021_paper.pdf)|[Code](https://github.com/NVlabs/AL-MDN)|
|2021| **ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation**   |ICCV|[Link](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_ReDAL_Region-Based_and_Diversity-Aware_Active_Learning_for_Point_Cloud_Semantic_ICCV_2021_paper.pdf)|-|
|2021| **Multi-Anchor Active Domain Adaptation for Semantic Segmentation**   |ICCV|[Link](https://openaccess.thecvf.com/content/ICCV2021/papers/Ning_Multi-Anchor_Active_Domain_Adaptation_for_Semantic_Segmentation_ICCV_2021_paper.pdf)|-|

### ICLR
| Year     | Title     | Venue | Paper | Code |
| :-------: | ------- | :-------: | :-------: | :-------: |
|2023| **Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle**   |ICLR|[Link](https://arxiv.org/abs/2105.14559)|[Code](https://github.com/jaeohwoo/balancedentropy)|
|2023| **A Simple Yet Powerful Deep Active Learning With Snapshots Ensembles**   |ICLR|[Link](https://openreview.net/pdf?id=IVESH65r0Ar)|-|

### ICML
| Year     | Title     | Venue | Paper | Code |
| :-------: | ------- | :-------: | :-------: | :-------: |
|2022| **Active Learning on a Budget: Opposite Strategies Suit High and Low Budgets**   |ICML|[Link](https://arxiv.org/abs/2202.02794)|[Code](https://github.com/avihu111/typiclust)|
|2022| **Constants Matter: The Performance Gains of Active Learning**   |ICML|[Link](https://proceedings.mlr.press/v162/mussmann22a/mussmann22a.pdf)|-|
